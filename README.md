# Neural Probabilistic Language Model (Portuguese Implementation)

Este repositÃ³rio contÃ©m minha implementaÃ§Ã£o do clÃ¡ssico paper [*A Neural Probabilistic Language Model*](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf), de Yoshua Bengio et al.

Ao contrÃ¡rio da maioria das implementaÃ§Ãµes disponÃ­veis, que usam o corpus em inglÃªs, aqui o modelo foi treinado usando **dados em portuguÃªs da Wikipedia** â€” o que traz desafios e vocabulÃ¡rio bastante diferentes!

## Resultados

### ğŸ§  O modelo aprendeu!
- **Loss de validaÃ§Ã£o** caiu de **4.99 â†’ 4.43** em 10 Ã©pocas.
- **Similaridade semÃ¢ntica** entre â€œreiâ€ e â€œrainhaâ€ aumentou de **-0.03 â†’ 0.21**.
- Palavras mais prÃ³ximas de â€œreiâ€ ao final do treinamento incluÃ­am:  
  `"imperador"`, `"papa"`, `"substituiu"` â€” o que mostra que o modelo aprendeu relaÃ§Ãµes contextuais coerentes.

### âš™ï¸ Treinamento
- Executado em CPU (sim, foi na forÃ§a da vontade rsrs)
- 10 Ã©pocas sobre o dataset tokenizado da Wikipedia em portuguÃªs.
- Vocab size, dimensÃ£o de embeddings e demais parÃ¢metros configurÃ¡veis no script.

## Requisitos

- Python 3.8+
- PyTorch
- tqdm
- numpy
- nltk

Instale os requisitos com:

```bash
pip install -r requirements.txt
